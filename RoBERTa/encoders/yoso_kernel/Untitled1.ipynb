{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "endless-understanding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/fast_hadamard_transform_kernel/build.ninja...\n",
      "Building extension module fast_hadamard_transform_kernel...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module fast_hadamard_transform_kernel...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/lsh_cumulation_kernel/build.ninja...\n",
      "Building extension module lsh_cumulation_kernel...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module lsh_cumulation_kernel...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/count_sort_kernel/build.ninja...\n",
      "Building extension module count_sort_kernel...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module count_sort_kernel...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/weighted_lsh_cumulation_kernel/build.ninja...\n",
      "Building extension module weighted_lsh_cumulation_kernel...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module weighted_lsh_cumulation_kernel...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import fast_hadamard_transform.kernel as fast_hadamard_transform\n",
    "import lsh_cumulation.kernel as lsh_cumulation\n",
    "import weighted_lsh_cumulation.count_sort.kernel as count_sort\n",
    "import weighted_lsh_cumulation.kernel as weighted_lsh_cumulation\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "contrary-cookbook",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n",
      "0.88\n",
      "2.29\n",
      "3.62\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    B = 1\n",
    "    S = 4096\n",
    "    H = 16\n",
    "    D = 64\n",
    "    num_buckets = S // 2\n",
    "    hashcode_len = int(math.log2(num_buckets))\n",
    "    num_part = int(H / (D / hashcode_len))\n",
    "\n",
    "    Q = torch.randn(B * 12, S, D).cuda().float()\n",
    "    K = torch.randn(B * 12, S, D).cuda().float()\n",
    "    V = torch.randn(B * 12, S, D).cuda().float()\n",
    "    mask = torch.ones(B * 12, S, dtype = torch.int32).cuda()\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.time()\n",
    "    for _ in range(100):\n",
    "        Dmat = fast_hadamard_transform.generate_Dmat(B, D, H, hashcode_len, device = Q.device)\n",
    "        Q_hash = fast_hadamard_transform.fast_hash(mask, Q, Dmat, H, hashcode_len)\n",
    "        K_hash = fast_hadamard_transform.fast_hash(mask, K, Dmat, H, hashcode_len)\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    \n",
    "    latency_hash = (t1 - t0) * 1000 / 100\n",
    "    print(round(latency_hash, 2))\n",
    "        \n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.time()\n",
    "    for _ in range(100):\n",
    "        result = lsh_cumulation.lsh_cumulation_query(mask, Q_hash, mask, K_hash, V, hashcode_len)\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    \n",
    "    latency_table = (t1 - t0) * 1000 / 100\n",
    "    print(round(latency_table, 2))\n",
    "    \n",
    "    latency_attn = latency_hash + latency_table\n",
    "    \n",
    "    ff = torch.nn.Linear(D * 12, D * 12).cuda()\n",
    "    X = torch.randn(B, 12, S, D).cuda().float()\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.time()\n",
    "    for _ in range(100):\n",
    "        X = ff(X.transpose(1, 2).reshape(B, S, D * 12))\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    \n",
    "    latency_dense = (t1 - t0) * 1000 / 100\n",
    "    print(round(latency_attn + latency_dense, 2))\n",
    "    \n",
    "    W_q = torch.nn.Linear(D * 12, D * 12).cuda()\n",
    "    W_k = torch.nn.Linear(D * 12, D * 12).cuda()\n",
    "    W_v = torch.nn.Linear(D * 12, D * 12).cuda()\n",
    "    X = torch.randn(B, S, D * 12).cuda().float()\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.time()\n",
    "    for _ in range(100):\n",
    "        Q, K, V = W_q(X), W_k(X), W_v(X)\n",
    "        Q = Q.reshape(B, S, 12, D).transpose(1, 2)\n",
    "        K = K.reshape(B, S, 12, D).transpose(1, 2)\n",
    "        V = V.reshape(B, S, 12, D).transpose(1, 2)\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "\n",
    "    latency_prep = (t1 - t0) * 1000 / 100\n",
    "    print(round(latency_prep + latency_attn + latency_dense, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "balanced-cement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.52\n",
      "15.04\n",
      "16.4\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    B = 1\n",
    "    S = 4096\n",
    "    D = 64\n",
    "    num_buckets = S // 2\n",
    "    hashcode_len = int(math.log2(num_buckets))\n",
    "    num_part = int(H / (D / hashcode_len))\n",
    "\n",
    "    Q = torch.randn(B * 12, S, D).cuda().float()\n",
    "    K = torch.randn(B * 12, S, D).cuda().float()\n",
    "    V = torch.randn(B * 12, S, D).cuda().float()\n",
    "    mask = torch.ones(B * 12, S, dtype = torch.int32).cuda()\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.time()\n",
    "    for _ in range(100):\n",
    "        dot = torch.matmul(Q, K.transpose(-1, -2)) / math.sqrt(D)\n",
    "        dot = dot - 1e6 * mask[:, None, :]\n",
    "        softmax = torch.nn.functional.softmax(dot, dim = -1)\n",
    "        out = torch.matmul(softmax, V)\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    \n",
    "    latency_attn = (t1 - t0) * 1000 / 100\n",
    "    print(round(latency_attn, 2))\n",
    "    \n",
    "    ff = torch.nn.Linear(D * 12, D * 12).cuda()\n",
    "    X = torch.randn(B, 12, S, D).cuda().float()\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.time()\n",
    "    for _ in range(100):\n",
    "        X = ff(X.transpose(1, 2).reshape(B, S, D * 12))\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    \n",
    "    latency_dense = (t1 - t0) * 1000 / 100\n",
    "    print(round(latency_attn + latency_dense, 2))\n",
    "    \n",
    "    W_q = torch.nn.Linear(D * 12, D * 12).cuda()\n",
    "    W_k = torch.nn.Linear(D * 12, D * 12).cuda()\n",
    "    W_v = torch.nn.Linear(D * 12, D * 12).cuda()\n",
    "    X = torch.randn(B, S, D * 12).cuda().float()\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.time()\n",
    "    for _ in range(100):\n",
    "        Q, K, V = W_q(X), W_k(X), W_v(X)\n",
    "        Q = Q.reshape(B, S, 12, D).transpose(1, 2)\n",
    "        K = K.reshape(B, S, 12, D).transpose(1, 2)\n",
    "        V = V.reshape(B, S, 12, D).transpose(1, 2)\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "\n",
    "    latency_prep = (t1 - t0) * 1000 / 100\n",
    "    print(round(latency_prep + latency_attn + latency_dense, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "composed-gossip",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.62\n",
      "1.22\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for b in [1, 2]:\n",
    "        B = b\n",
    "        S = 4096\n",
    "        D = 768\n",
    "\n",
    "        ff = torch.nn.Linear(D, D).cuda()\n",
    "        X = torch.randn(B, 12, S, 64).cuda().float()\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        t0 = time.time()\n",
    "        for _ in range(100):\n",
    "            X = ff(X.transpose(1, 2).reshape(B, S, D))\n",
    "        torch.cuda.synchronize()\n",
    "        t1 = time.time()\n",
    "\n",
    "        print(round((t1 - t0) * 1000 / 100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "incorrect-little",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.21, 0.61, 0.73)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.21, 0.21+0.4, 0.21+0.4+0.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "assured-projector",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29\n",
      "0.49\n",
      "7.74\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for b in [1, 2, 32]:\n",
    "        B = b\n",
    "        S = 512\n",
    "        D = 64\n",
    "\n",
    "        Q = torch.randn(B, 12, S, D).cuda().float()\n",
    "        K = torch.randn(B, 12, S, D).cuda().float()\n",
    "        V = torch.randn(B, 12, S, D).cuda().float()\n",
    "        mask = torch.ones(B, S, dtype = torch.int32).cuda()\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        t0 = time.time()\n",
    "        for _ in range(100):\n",
    "            dot = torch.matmul(Q, K.transpose(-1, -2)) / math.sqrt(D)\n",
    "            dot = dot - 1e6 * mask[:, None, None, :]\n",
    "            softmax = torch.nn.functional.softmax(dot, dim = -1)\n",
    "            out = torch.matmul(softmax, V)\n",
    "        torch.cuda.synchronize()\n",
    "        t1 = time.time()\n",
    "\n",
    "        print(round((t1 - t0) * 1000 / 100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "smooth-burner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2415625, 0.41781250000000003, 0.483125)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7.73/32, (7.73+5.64)/32, (7.73+5.64+2.09)/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common-imaging",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
